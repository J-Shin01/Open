{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffaf7d19-5e0e-48a4-914a-dc28a83b5635",
   "metadata": {},
   "source": [
    "# HW7 과제 : 2021741022 신정석호\n",
    "목표\n",
    "텍스트 데이터를 표현하는 여러 가지 방법\n",
    "- (Document-Term Matrix, TF-IDF Matrix, Word2Vec, Pre-Trained GloVe-Twitter-25)을 사용하여 \n",
    "- 20news_group 데이터셋을 분류하고, 각 방법의 성능을 비교 및 분석한다\n",
    "\n",
    "과제 설명\n",
    "- 1.데이터 : fetch_20newsgroups 데이터셋을 찾아 로드한다.\n",
    "- 2.Text Data 표현 방법을 다음의 4 가지를 이용하여 각각 분류 성능을 구하고 비교한다. \n",
    "- (분류 모델은 선형 및 대표적인 비선형 모델 몇 가지를 선택하고, 성능은 Accuray, F-1 Score, ROC/AUC 사용한다.)\n",
    "- Document-Term Matrix (DTM):\n",
    "- TF-IDF Matrix:\n",
    "- Word2Vec() 을 이용하여 직접 임베딩 (시간이 많이 걸릴 수 있음)\n",
    "- Pre-Trained GloVe-Twitter-25 사용 \n",
    "- **데이터 로드 및 Text 변환 등은 제공해 준 소스 코드 (Topic Modeling 예제 소스)를 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0b78f-a77c-4e93-a10a-f61db0488bb8",
   "metadata": {},
   "source": [
    "# 0. 데이터 구조 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa545656-e54b-4ea4-8c18-b043096fac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# fetch_20newsgroups는 미국 뉴스그룹(뉴스 게시판) 글들로 구성된 텍스트 데이터셋을 제공합니다.\n",
    "# 정확히는 Usenet이라는 인터넷 게시판에 올라온 글들을 20개의 주제로 분류한 데이터셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810cfcee-a4df-4002-9cbd-89ee9cea8c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "# 20 Newsgroups는 train과 test 로 나뉘어 있다.\n",
    "# headers(이메일, 제목, 발신자, 날짜같은 메타데이터( = 본문을 설명하는 정보 등) ) \n",
    "# footers ( 글 마지막에 붙는 서명 ) \n",
    "# quotes ( 이전 글을 인용한 부분 ) \n",
    "\n",
    "documents = data.data # 실제 텍스트 데이터\n",
    "labels = data.target # 텍스트 분류 모델의 정답 라벨(y)\n",
    "\n",
    "\n",
    "print(data.keys())\n",
    "# data : 문서 본문 텍스트 리스트\n",
    "# filenames : 각 문서가 원래 저장되어 있던 로컬 경로. 원본 데이터셋에서 해당 글이 어떤 파일에서 왔는지 알려줌 \n",
    "# target_names : 20개의 카테고리 이름\n",
    "# target : 각 문서의 라벨(숫자, 0-19). 텍스트 분류 모델의 정답 라벨(y). target_names의 index에 해당한다.\n",
    "# DESCR : 데이터셋의 설명문\n",
    "\n",
    "# 예시 : data.data[0]의 카테고리는 data.target[0]이다.\n",
    "# data.target_names[data.target[0]]은 데이터의 카테고리 이름이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b610d9-27f1-421b-a0fa-707565480fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 구조 키: dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "총 문서 수: 11314\n",
      "카테고리 수: 20\n",
      "카테고리 이름들: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구조 확인\n",
    "print(\"데이터 구조 키:\", data.keys())\n",
    "print(\"총 문서 수:\", len(data.data))\n",
    "print(\"카테고리 수:\", len(data.target_names))\n",
    "print(\"카테고리 이름들:\", data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d19bb31-5bc5-4503-ac59-0d91eacec7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]  I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "[1]  A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "[2]  well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "\n",
      "[3]  \n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "\n",
      "[4]  From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'[{i}]  {data.data[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9042ca-d36b-473f-bc47-b24f35499aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ... 3 1 8]\n",
      "Labels(y) shape: (11314,)\n",
      "예시 라벨: [ 7  4  4  1 14 16 13  3  2  4]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(\"Labels(y) shape:\", labels.shape)\n",
    "print(\"예시 라벨:\", labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92dcc2e-57ee-499e-be72-e52e393e58b5",
   "metadata": {},
   "source": [
    "# 1. 텍스트 전처리\n",
    "- 소문자 변환\n",
    "- 불용어 제거\n",
    "- 특수문자/숫자 제거\n",
    "- 토근화 ( 문장을 단어 단위로 쪼개기)\n",
    "- 어간 추출/ 표제어 추출 ( 단어 형태 통일. ex: running -> run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48ec60d-6337-45f1-ad68-2d66bac9379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\c\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\c\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\c\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\c\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\c\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\c\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493498dc-71ee-4825-bffa-43437e92a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  # 토큰화 함수\n",
    "from nltk.corpus import stopwords # 불용어 제공\n",
    "import nltk\n",
    "import re # 정규 표현식 라이브러리 (특수문자제거, 숫자 제거, 문장 부호 제거등 가능)\n",
    "\n",
    "nltk.download('punkt_tab') # 토큰화 도구 다운로드 \n",
    "nltk.download('stopwords') # 불용어 목록 다운로드\n",
    "\n",
    "# 불용어 정의\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ae23ca-f4ea-410b-93ca-bfe036330eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 특수문자 제거 (알파벳만 남김)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # 소문자(a~z)와 공백(\\s) 이외의 모든 문자\n",
    "    # 토큰화\n",
    "    tokens = word_tokenize(text)\n",
    "    # 불용어 제거\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 4. 모든 문서 전처리\n",
    "processed_docs = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac8b92f-264d-44f6-9ca7-bc7bed5ca603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'email']\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[0]) # 전처리된 1번 텍스트 데이터 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f67495ad-daac-43e2-aadb-bb7edecf255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i in range(20) :\n",
    "    print(data.target_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91dbf1-fb60-4b09-bfa2-5cba0de77a78",
   "metadata": {},
   "source": [
    "# 2.특성 추출\n",
    "- DTM\n",
    "- TF-IDF Matrix\n",
    "- Word2Vec\n",
    "- Pre-trined Glove(Twitter-25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaad56d9-619c-458f-a0d5-93b866bc7284",
   "metadata": {},
   "source": [
    "## 1). Document_Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85efb66f-b9a8-4a19-9c51-3c18b26b4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM shape: (11314, 5000)\n",
      "단어 예시: ['aa' 'aaa' 'ab' 'abc' 'ability' 'able' 'abort' 'abortion' 'abs' 'absence'\n",
      " 'absolute' 'absolutely' 'abstract' 'absurd' 'abuse' 'abuses' 'ac'\n",
      " 'academic' 'acceleration' 'accelerator' 'accept' 'acceptable'\n",
      " 'acceptance' 'accepted' 'accepting' 'access' 'accident' 'accidental'\n",
      " 'accidents' 'accomplish' 'accomplished' 'according' 'account' 'accounts'\n",
      " 'accuracy' 'accurate' 'accused' 'achieve' 'achieved' 'acid' 'acknowledge'\n",
      " 'aclu' 'acquire' 'acquired' 'act' 'acting' 'action' 'actions' 'active'\n",
      " 'actively' 'activities' 'activity' 'acts' 'actual' 'actually' 'ad' 'adam'\n",
      " 'adams' 'adaptec' 'adapter' 'add' 'added' 'adding' 'addition'\n",
      " 'additional' 'address' 'addressed' 'addresses' 'addressing' 'adds'\n",
      " 'adequate' 'adirondack' 'adjust' 'adl' 'administration' 'administrative'\n",
      " 'administrator' 'admit' 'admitted' 'adobe' 'adopted' 'ads' 'adult'\n",
      " 'adults' 'advance' 'advanced' 'advantage' 'advantages' 'advertising'\n",
      " 'advice' 'advocate' 'aerospace' 'affairs' 'affect' 'affected' 'afford'\n",
      " 'afraid' 'african' 'afternoon' 'agdam']\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 27 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 4921)\t1\n",
      "  (0, 625)\t4\n",
      "  (0, 3917)\t1\n",
      "  (0, 1112)\t1\n",
      "  (0, 1335)\t1\n",
      "  (0, 4231)\t1\n",
      "  (0, 2603)\t1\n",
      "  (0, 2468)\t1\n",
      "  (0, 1381)\t1\n",
      "  (0, 595)\t1\n",
      "  (0, 1336)\t1\n",
      "  (0, 3627)\t1\n",
      "  (0, 4147)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 4011)\t1\n",
      "  (0, 3776)\t1\n",
      "  (0, 487)\t1\n",
      "  (0, 2434)\t1\n",
      "  (0, 2839)\t1\n",
      "  (0, 1458)\t1\n",
      "  (0, 4212)\t1\n",
      "  (0, 4983)\t1\n",
      "  (0, 3458)\t1\n",
      "  (0, 2052)\t1\n",
      "  (0, 2214)\t1\n",
      "  (0, 2604)\t1\n",
      "  (0, 1432)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1.CountVectorizer 객체 생성\n",
    "# 공백으로 토큰 합쳐줄 것이다.\n",
    "# max_features : 상위 n개 단어만 사용한다. 차원 축소다.\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# 전처리 토큰 리스트를 문자열로 변환\n",
    "# \"word1 word2 word3 ...\" 형태로 만들어야 DTM 변환 가능\n",
    "processed_docs_text_DTM = [\" \".join(doc) for doc in processed_docs]\n",
    "\n",
    "# DTM 생성\n",
    "X_dtm = vectorizer.fit_transform(processed_docs_text_DTM)\n",
    "\n",
    "print(\"DTM shape:\", X_dtm.shape) # X_dtm.shape → (문서 수, 단어 수)\n",
    "print(\"단어 예시:\", vectorizer.get_feature_names_out()[:100]) \n",
    "# vectorizer.get_feature_names_out() → DTM에 포함된 단어 목록 확인\n",
    "print(X_dtm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e086a-efa7-47d0-8344-32409bd83c5e",
   "metadata": {},
   "source": [
    "## 2). TF-IDF Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae68198-723c-4072-a42d-08f6832b7249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (11314, 5000)\n",
      "단어 예시: ['aa' 'aaa' 'ab' 'abc' 'ability' 'able' 'abort' 'abortion' 'abs' 'absence']\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 27 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 4921)\t0.16873351038537243\n",
      "  (0, 625)\t0.5770428165281328\n",
      "  (0, 3917)\t0.15625122179382797\n",
      "  (0, 1112)\t0.13176485822537468\n",
      "  (0, 1335)\t0.18383353104247055\n",
      "  (0, 4231)\t0.20468916920700245\n",
      "  (0, 2603)\t0.16646417577540215\n",
      "  (0, 2468)\t0.170784119685386\n",
      "  (0, 1381)\t0.15588233225760645\n",
      "  (0, 595)\t0.1313562529018362\n",
      "  (0, 1336)\t0.21257256192951604\n",
      "  (0, 3627)\t0.11031449781535926\n",
      "  (0, 4147)\t0.1402247281544394\n",
      "  (0, 63)\t0.17317734259832754\n",
      "  (0, 4011)\t0.17980085800262652\n",
      "  (0, 3776)\t0.14996697456861016\n",
      "  (0, 487)\t0.15977177593639041\n",
      "  (0, 2434)\t0.08558426061403875\n",
      "  (0, 2839)\t0.16089162215486885\n",
      "  (0, 1458)\t0.17759906408487572\n",
      "  (0, 4212)\t0.20063576354399884\n",
      "  (0, 4983)\t0.1151511208274027\n",
      "  (0, 3458)\t0.20063576354399884\n",
      "  (0, 2052)\t0.1538666775895261\n",
      "  (0, 2214)\t0.14769763995863988\n",
      "  (0, 2604)\t0.13112502308803076\n",
      "  (0, 1432)\t0.1187034270232619\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. 토큰 리스트를 문자열로 합치기\n",
    "processed_docs_text_tfidf = [\" \".join(doc) for doc in processed_docs]\n",
    "\n",
    "# 2. TfidfVectorizer 객체 생성\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# 3. TF-IDF 행렬 생성 (학습 + 변환)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(processed_docs_text_tfidf)\n",
    "\n",
    "# 4. 결과 확인\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "print(\"단어 예시:\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# 5. TF-IDF 행렬 확인 (희소행렬)\n",
    "print(X_tfidf[0])  # 첫 번째 문서 TF-IDF 값 확인\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811b234-139a-4044-8146-252fb7713e5e",
   "metadata": {},
   "source": [
    "## 3). Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94b1b017-f064-4700-9942-09be744575ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\c\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\c\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\c\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\c\\anaconda3\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\c\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b8c7a0-db88-4374-8a75-98a283a8c623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 크기: 100\n",
      "단어 수: 26858\n",
      "[-0.11656905  0.41043544 -0.27389303  0.5141487   0.20520997  0.33602285\n",
      "  0.47458583  0.58263767 -0.22269954 -0.1593332   0.49216047  0.03573945\n",
      "  0.4538404  -0.3135477   0.17525211 -0.05746875  0.31972495  0.328339\n",
      "  0.09182618 -0.5050486   0.30983326  0.09905732  0.14972566 -0.21949087\n",
      " -0.13248317  0.31774575  0.3078806   0.13189791 -0.30103016 -0.50623727\n",
      "  0.60136956 -0.00767644  0.6557146   0.0778475  -0.14553173  0.26611197\n",
      " -0.22611283 -0.10846872 -0.26990446 -0.5599226   0.6691951  -0.1465858\n",
      "  0.3682958  -0.1742139   0.29297087 -0.01009589 -0.1086015  -0.13453941\n",
      " -0.6825804  -0.03811453  0.29693714 -0.26676676  0.04211111  0.01508519\n",
      "  0.02648553 -0.5527223   0.04759199 -0.18908085 -0.07902213 -0.6600583\n",
      " -0.39856854 -0.17602691  0.59625274 -0.18878831 -0.42617935  0.26071268\n",
      "  0.02611624  0.00871262 -0.2880952  -0.01381291 -0.07645959  0.31380916\n",
      "  0.03320921  0.7369341  -0.23880802  0.04197558  0.09309804 -0.17182995\n",
      " -0.2363331  -0.12555015  0.170633   -0.44528407 -0.13561386  0.128179\n",
      "  0.0246978   0.01705072  0.32526013 -0.30626518 -0.43148425  0.32057709\n",
      " -0.09475978  0.13360135  0.10417771 -0.17381653  0.58201534  0.578756\n",
      "  0.50014067  0.07957062  0.02808716 -0.26276356]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=processed_docs,  # 토큰 리스트\n",
    "    vector_size=100,           # 임베딩 차원 (과제면 100~300)\n",
    "    window=5,                  # 윈도우 사이즈\n",
    "    min_count=3,               # 최소 등장 횟수\n",
    "    workers=4,                 # CPU 병렬 처리\n",
    "    sg=1                       # 1 = Skip-gram, 0 = CBOW\n",
    ")\n",
    "print(\"벡터 크기:\", w2v_model.vector_size)\n",
    "print(\"단어 수:\", len(w2v_model.wv))\n",
    "\n",
    "# 예시 단어 벡터 보기\n",
    "print(w2v_model.wv['know'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9578c69e-c6cc-4b25-8e48-c2692756f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서 벡터 만들기 : 문서 벡터를 통해서 카테고리를 유추하겠다.\n",
    "def doc_vector(tokens): # 입력 받는 tokens는 document의 text data\n",
    "    tokens = [t for t in tokens if t in w2v_model.wv] # text data의 단어가 w2v_model에 있으면 tokens에 넣는다.\n",
    "    # print(tokens)\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(w2v_model.wv[tokens], axis=0) # 여기서 token은 text data에 있는 단어 벡터 모음이다. # return 값은 문서 벡터\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "697d6700-68d1-4657-b48b-6c5c571400b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38090837  0.47254038  0.19713055  0.13550444  0.20539479  0.11761735\n",
      "  0.36432046  0.00391385 -0.12137806  0.01118286  0.12410118 -0.34633318\n",
      "  0.18765333 -0.07863998  0.11726198 -0.09726462 -0.11631639 -0.1288176\n",
      " -0.16996819 -0.35518011  0.06535943  0.14309473  0.22254595 -0.16164674\n",
      "  0.09607749  0.21468958  0.00904886 -0.31106719 -0.01309814 -0.27859268\n",
      "  0.27958462 -0.14930423  0.24738187  0.22641945  0.03664828  0.24197109\n",
      "  0.07190105  0.11463904  0.02962998 -0.41165152  0.03325654  0.05039596\n",
      "  0.06314963 -0.03747498 -0.07423417 -0.14692268  0.00191341  0.01045698\n",
      " -0.19523375  0.04333508  0.21030042  0.04911361 -0.03937061 -0.12493189\n",
      "  0.16554125 -0.24166024  0.1053234   0.08561282  0.00999474 -0.23495927\n",
      " -0.24523336 -0.21143208  0.27546737 -0.09220283 -0.20348239  0.06868379\n",
      " -0.04843501  0.20218126 -0.32499114  0.17203185 -0.16800328  0.25248504\n",
      "  0.11421384  0.17818326 -0.05849096 -0.16721199 -0.05849905 -0.13178584\n",
      " -0.29132378 -0.01516002 -0.11739565 -0.07039081  0.03022994  0.21289717\n",
      "  0.06356844  0.05114689 -0.04021995 -0.0115343  -0.03184343  0.04064576\n",
      " -0.09253837  0.03277136  0.15669799  0.03076276  0.45314923  0.31737888\n",
      "  0.20182623  0.13490541  0.15921932 -0.18674202]\n"
     ]
    }
   ],
   "source": [
    "X_w2v = np.array([doc_vector(doc) for doc in processed_docs])\n",
    "print(X_w2v[0]) # 첫번째 text data의 문서 벡터이다.  앞서서 100차원이라고 설정했으니 100개이다.\n",
    "# 이 문서를 표현하는 벡터라고 보면 된다.\n",
    "# 벡터끼리 가까우면 문장의 주제가 비슷하다고 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c495bc0-ef0a-44b3-b722-e5ebc5782373",
   "metadata": {},
   "source": [
    "## 4). Pre-trined Glove(Twitter-25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1405995c-53d4-478d-8867-3192f9f4f187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe 파일 이미 존재: glove.twitter.27B\\glove.twitter.27B.25d.txt\n",
      "압축 해제된 파일 목록:\n",
      "['glove.twitter.27B.100d.txt', 'glove.twitter.27B.200d.txt', 'glove.twitter.27B.25d.txt', 'glove.twitter.27B.50d.txt']\n"
     ]
    }
   ],
   "source": [
    "# Pre-trined Glove(Twitter-25) 사용\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "\n",
    "url = \"https://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
    "zip_path = \"glove.twitter.27B.zip\"\n",
    "glove_dir = \"glove.twitter.27B\"\n",
    "glove_file = os.path.join(glove_dir, \"glove.twitter.27B.25d.txt\")\n",
    "\n",
    "# 1. 파일 존재 여부 확인\n",
    "\n",
    "if os.path.exists(glove_file):\n",
    "    print(f\"GloVe 파일 이미 존재: {glove_file}\")\n",
    "else:\n",
    "    # 2. 다운로드 진행 함수\n",
    "    def show_progress(block_num, block_size, total_size):\n",
    "        downloaded = block_num * block_size\n",
    "        percent = downloaded / total_size * 100\n",
    "        percent = min(100, percent)\n",
    "        sys.stdout.write(f\"\\r다운로드 진행: {percent:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # 3. 다운로드\n",
    "    print(\"GloVe 다운로드 시작 (약 1GB)\")\n",
    "    urllib.request.urlretrieve(url, zip_path, reporthook=show_progress)\n",
    "    print(\"\\n다운로드 완료!\")\n",
    "\n",
    "\n",
    "    # 4. 압축 해제\n",
    "    if not os.path.exists(glove_dir):\n",
    "        print(\"압축 해제 중...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(glove_dir)\n",
    "        print(\"압축 해제 완료!\")\n",
    "\n",
    "\n",
    "# 5. 확인\n",
    "print(\"압축 해제된 파일 목록:\")\n",
    "print(os.listdir(glove_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d305f380-e755-458f-b4e5-2c86b83a1a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe 파일 로딩 중...\n",
      "단어 수: 1193514\n"
     ]
    }
   ],
   "source": [
    "# 2. GloVe 25d 벡터 로드\n",
    "glove_file = os.path.join(glove_dir, \"glove.twitter.27B.25d.txt\")\n",
    "glove_dict = {}\n",
    "\n",
    "print(\"GloVe 파일 로딩 중...\")\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        glove_dict[word] = vector\n",
    "print(f\"단어 수: {len(glove_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3ec9f4-8a02-44ac-8d7c-c3c52a5e11a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 25)\n",
      "[[-0.17667596  0.32242805  0.14661244 ...  0.08981314 -0.10036357\n",
      "  -0.47160122]\n",
      " [ 0.07201795  0.37005344 -0.00142608 ...  0.08333968  0.10176188\n",
      "  -0.61223906]\n",
      " [ 0.03852868  0.50571251  0.11323483 ...  0.10709827  0.08945512\n",
      "  -0.38875204]\n",
      " ...\n",
      " [-0.23112494  0.19221558 -0.00541638 ...  0.64127189  0.25323161\n",
      "  -0.55703974]\n",
      " [ 0.22007486  0.17631085 -0.27827433 ...  0.15954717  0.16790809\n",
      "  -0.54380441]\n",
      " [-0.37664622  0.23516592  0.27163815 ...  0.46414351 -0.2559562\n",
      "  -0.11672594]]\n"
     ]
    }
   ],
   "source": [
    "# 3. 문서 벡터 생성 함수\n",
    "def doc_vector_glove(tokens):\n",
    "    vectors = [glove_dict[t] for t in tokens if t in glove_dict]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(25)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# 4. 전체 문서 변환\n",
    "X_glove = np.array([doc_vector_glove(doc) for doc in processed_docs])\n",
    "\n",
    "print(X_glove.shape)  # (문서 수, 25)\n",
    "print(X_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da448e6-7227-4254-9277-286baec9fc26",
   "metadata": {},
   "source": [
    "# 3. 모델별 학습 및 평가\n",
    "- * 학습 데이터\n",
    "- Document_Term Matrix : X_dtm (BOW 형태)\n",
    "- TF-idf : X_tfidf (BOW 형태)\n",
    "- Word2Vec : X_w2v (문서벡터형)\n",
    "- Twitter-25 : X_glove (문서벡터형)\n",
    "- 결과 데이터 : labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4be6e2a-da15-4be6-a287-fa87d8746c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (Document-Term Matrix) ===\n",
      "Accuracy: 0.6597, F1-score: 0.6613, ROC/AUC: 0.9399678154833249\n",
      "\n",
      "=== Logistic Regression (TF-IDF) ===\n",
      "Accuracy: 0.7004, F1-score: 0.6983, ROC/AUC: 0.9587913082321766\n",
      "\n",
      "=== Logistic Regression (Word2Vec Document Vectors) ===\n",
      "Accuracy: 0.5943, F1-score: 0.5797, ROC/AUC: 0.9366235892058066\n",
      "\n",
      "=== Logistic Regression (GloVe Twitter-25 Vectors) ===\n",
      "Accuracy: 0.4180, F1-score: 0.3976, ROC/AUC: 0.8782381195283273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# full class 확장용 함수 (ROC/AUC 에러 방지)\n",
    "def expand_proba_to_full_classes(y_proba, trained_classes, full_classes):\n",
    "    full = np.zeros((y_proba.shape[0], len(full_classes)))\n",
    "    for i, c in enumerate(trained_classes):\n",
    "        idx = np.where(full_classes == c)[0][0]\n",
    "        full[:, idx] = y_proba[:, i]\n",
    "    return full\n",
    "\n",
    "\n",
    "# 1. 학습/테스트 분리 (80% train / 20% test)\n",
    "X_dtm_train, X_dtm_test, y_train, y_test = train_test_split(X_dtm, labels, test_size=0.2, random_state=42)\n",
    "X_tfidf_train, X_tfidf_test, trash_val1, trash_val2 = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
    "X_w2v_train, X_w2v_test, trash_val3, trash_val4 = train_test_split(X_w2v, labels, test_size=0.2, random_state=42)\n",
    "X_glove_train, X_glove_test, trash_val5, trash_val6 = train_test_split(X_glove, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# trash_val은 버리는 값이다.\n",
    "# y_train하고 y_test와 동일하게 사용하기 때문에 trash_val로 표기했다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Logistic Regression 학습/평가 함수\n",
    "\n",
    "def train_and_evaluate_lr(X_train, X_test, y_train, y_test, name):\n",
    "    print(f\"=== Logistic Regression ({name}) ===\")\n",
    "    \n",
    "    # OneVsRestClassifier로 다중 클래스 처리\n",
    "    clf = OneVsRestClassifier(LogisticRegression(max_iter=5000, random_state=42))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Accuracy & F1-score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # ROC/AUC 계산\n",
    "    try:\n",
    "        y_proba = clf.predict_proba(X_test)\n",
    "        # full class 확장\n",
    "        y_full = expand_proba_to_full_classes(y_proba, clf.classes_, np.unique(y_test))\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        roc = roc_auc_score(y_test_bin, y_full, multi_class='ovr')\n",
    "    except:\n",
    "        roc = None\n",
    "\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, F1-score: {f1:.4f}, ROC/AUC: {roc if roc is not None else 'N/A'}\\n\")\n",
    "    return clf, acc, f1, roc\n",
    "\n",
    "\n",
    "# 학습/평가 및 결과 저장\n",
    "acc_lr = [0]*4\n",
    "f1_lr  = [0]*4\n",
    "roc_lr = [0]*4\n",
    "\n",
    "clf_dtm_lr, acc_lr[0], f1_lr[0], roc_lr[0] = train_and_evaluate_lr(X_dtm_train, X_dtm_test, y_train, y_test, \"Document-Term Matrix\")\n",
    "clf_tfidf_lr, acc_lr[1], f1_lr[1], roc_lr[1] = train_and_evaluate_lr(X_tfidf_train, X_tfidf_test, y_train, y_test, \"TF-IDF\")\n",
    "clf_w2v_lr, acc_lr[2], f1_lr[2], roc_lr[2] = train_and_evaluate_lr(X_w2v_train, X_w2v_test, y_train, y_test, \"Word2Vec Document Vectors\")\n",
    "clf_glove_lr, acc_lr[3], f1_lr[3], roc_lr[3] = train_and_evaluate_lr(X_glove_train, X_glove_test, y_train, y_test, \"GloVe Twitter-25 Vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbfe61c6-78b7-45dc-86ed-05fbd13fcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear SVM (Document-Term Matrix) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5979, F1-score: 0.5985, ROC/AUC: 0.9055547729662374\n",
      "\n",
      "=== Linear SVM (TF-IDF) ===\n",
      "Accuracy: 0.6880, F1-score: 0.6882, ROC/AUC: 0.9406237674065961\n",
      "\n",
      "=== Linear SVM (Word2Vec Document Vectors) ===\n",
      "Accuracy: 0.6023, F1-score: 0.5883, ROC/AUC: 0.9244338706474254\n",
      "\n",
      "=== Linear SVM (GloVe Twitter-25 Vectors) ===\n",
      "Accuracy: 0.3981, F1-score: 0.3675, ROC/AUC: 0.8594266699358496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# Linear SVM 학습/평가 함수\n",
    "def train_and_evaluate_svc(X_train, X_test, y_train, y_test, name):\n",
    "    print(f\"=== Linear SVM ({name}) ===\")\n",
    "    clf = LinearSVC(max_iter=5000, random_state=42)  # max_iter 늘려서 수렴 문제 방지\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Accuracy & F1-score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # ROC/AUC 계산\n",
    "    try:\n",
    "        scores = clf.decision_function(X_test)\n",
    "\n",
    "        # decision_function shape이 (n, classes) 라면 그대로\n",
    "        # 만약 일부 클래스만 있으면 full class 확장 필요\n",
    "        if scores.shape[1] != len(np.unique(y_test)):\n",
    "            scores = expand_proba_to_full_classes(scores, clf.classes_, np.unique(y_test))\n",
    "\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        roc = roc_auc_score(y_test_bin, scores, multi_class='ovr')\n",
    "    except:\n",
    "        roc = None\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, F1-score: {f1:.4f}, ROC/AUC: {roc if roc is not None else 'N/A'}\\n\")\n",
    "    return clf, acc, f1, roc\n",
    "\n",
    "# 학습/평가 및 결과 저장\n",
    "acc_svc = [0]*4\n",
    "f1_svc  = [0]*4\n",
    "roc_svc = [0]*4\n",
    "\n",
    "clf_dtm_svc, acc_svc[0], f1_svc[0], roc_svc[0] = train_and_evaluate_svc(X_dtm_train, X_dtm_test, y_train, y_test, \"Document-Term Matrix\")\n",
    "clf_tfidf_svc, acc_svc[1], f1_svc[1], roc_svc[1] = train_and_evaluate_svc(X_tfidf_train, X_tfidf_test, y_train, y_test, \"TF-IDF\")\n",
    "clf_w2v_svc, acc_svc[2], f1_svc[2], roc_svc[2] = train_and_evaluate_svc(X_w2v_train, X_w2v_test, y_train, y_test, \"Word2Vec Document Vectors\")\n",
    "clf_glove_svc, acc_svc[3], f1_svc[3], roc_svc[3] = train_and_evaluate_svc(X_glove_train, X_glove_test, y_train, y_test, \"GloVe Twitter-25 Vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533bcdd7-0f7f-4358-b14b-81c1fa9f473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest (Document-Term Matrix) ===\n",
      "Accuracy: 0.6200, F1-score: 0.6147, ROC/AUC: 0.9344713803905217\n",
      "\n",
      "=== Random Forest (TF-IDF) ===\n",
      "Accuracy: 0.6253, F1-score: 0.6206, ROC/AUC: 0.93761605110412\n",
      "\n",
      "=== Random Forest (Word2Vec Document Vectors) ===\n",
      "Accuracy: 0.5776, F1-score: 0.5723, ROC/AUC: 0.9369543842429928\n",
      "\n",
      "=== Random Forest (GloVe Twitter-25 Vectors) ===\n",
      "Accuracy: 0.4264, F1-score: 0.4178, ROC/AUC: 0.8922263794904282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest 학습/평가 함수\n",
    "def train_and_evaluate_rf(X_train, X_test, y_train, y_test, name):\n",
    "    print(f\"=== Random Forest ({name}) ===\")\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Accuracy & F1-score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # ROC/AUC (predict_proba 가능)\n",
    "    try:\n",
    "        y_proba = clf.predict_proba(X_test)\n",
    "        y_full = expand_proba_to_full_classes(y_proba, clf.classes_, np.unique(y_test))\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
    "        roc = roc_auc_score(y_test_bin, y_full, multi_class='ovr')\n",
    "    except:\n",
    "        roc = None\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}, F1-score: {f1:.4f}, ROC/AUC: {roc if roc is not None else 'N/A'}\\n\")\n",
    "    return clf, acc, f1, roc\n",
    "\n",
    "# 학습/평가 및 결과 저장\n",
    "acc_rf = [0]*4\n",
    "f1_rf  = [0]*4\n",
    "roc_rf = [0]*4\n",
    "\n",
    "clf_dtm_rf, acc_rf[0], f1_rf[0], roc_rf[0] = train_and_evaluate_rf(X_dtm_train, X_dtm_test, y_train, y_test, \"Document-Term Matrix\")\n",
    "clf_tfidf_rf, acc_rf[1], f1_rf[1], roc_rf[1] = train_and_evaluate_rf(X_tfidf_train, X_tfidf_test, y_train, y_test, \"TF-IDF\")\n",
    "clf_w2v_rf, acc_rf[2], f1_rf[2], roc_rf[2] = train_and_evaluate_rf(X_w2v_train, X_w2v_test, y_train, y_test, \"Word2Vec Document Vectors\")\n",
    "clf_glove_rf, acc_rf[3], f1_rf[3], roc_rf[3] = train_and_evaluate_rf(X_glove_train, X_glove_test, y_train, y_test, \"GloVe Twitter-25 Vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b01899a-6a1e-4956-8a72-20887cc2cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vectorizer    LR_Acc     LR_F1    LR_ROC   SVM_Acc    SVM_F1   SVM_ROC  \\\n",
      "0        DTM  0.659744  0.661320  0.939968  0.597879  0.598550  0.905555   \n",
      "1     TF-IDF  0.700398  0.698314  0.958791  0.688025  0.688193  0.940624   \n",
      "2   Word2Vec  0.594344  0.579709  0.936624  0.602298  0.588319  0.924434   \n",
      "3   GloVe-25  0.418029  0.397597  0.878238  0.398144  0.367462  0.859427   \n",
      "\n",
      "     RF_Acc     RF_F1    RF_ROC  \n",
      "0  0.619973  0.614706  0.934471  \n",
      "1  0.625276  0.620623  0.937616  \n",
      "2  0.577552  0.572346  0.936954  \n",
      "3  0.426425  0.417827  0.892226  \n"
     ]
    }
   ],
   "source": [
    "# 4가지 벡터 방식 이름\n",
    "vector_names = [\"DTM\", \"TF-IDF\", \"Word2Vec\", \"GloVe-25\"]\n",
    "\n",
    "# 각 모델별 성능 저장\n",
    "results = {\n",
    "    \"Vectorizer\": vector_names,\n",
    "    \n",
    "    \"LR_Acc\": acc_lr,\n",
    "    \"LR_F1\": f1_lr,\n",
    "    \"LR_ROC\": roc_lr,\n",
    "\n",
    "    \"SVM_Acc\": acc_svc,\n",
    "    \"SVM_F1\": f1_svc,\n",
    "    \"SVM_ROC\": roc_svc,\n",
    "    \n",
    "    \"RF_Acc\": acc_rf,\n",
    "    \"RF_F1\": f1_rf,\n",
    "    \n",
    "    \"RF_ROC\": roc_rf,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e8da2-0a92-4922-a7d0-d2b501801f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a02e46-6f68-416e-a6db-5df0ce5d6073",
   "metadata": {},
   "source": [
    "# 4.모델 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4cee2-9151-4af2-ad4e-614507eacba8",
   "metadata": {},
   "source": [
    "- DTM : Logistic Regression > Random Forest > SVM\n",
    "- TF-IDF : Logistic Regression > SVM > Random Forest\n",
    "- Word2Vec : Random Forest > Logistic Regression > SVM\n",
    "- Glove-25 : Random Forest > Logistic Regression > SVM\n",
    "\n",
    "- 대략 TF-IDF > DTM > Word2Vec > GloVe-25 (DTM 하고 Word2Vec 비슷하다)\n",
    "\n",
    "- 우선, 선형 모델에 비해 Random Forest는 비교적 안정적이나 선형 모델 대비 정확도가 낮다.\n",
    "- Word2Vec, GloVe-25: 차원이 낮아 정보 손실, 단어 임베딩 기반은 더 큰 차원일 때 성능 기대 가능\n",
    "\n",
    "- 가장 좋은 조합은 TF-IDF 와 Logistic Regression 혹은 Linear SVM 조합이다.\n",
    "- (Accuray / F1 이 최고 성능을 보이고, ROC/AUC 도 높다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99539ff-c0dd-4282-b3cf-26b983ec11f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
